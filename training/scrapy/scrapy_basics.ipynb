{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy documentation\n",
    "\n",
    "Scrapy is a fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages.\n",
    "\n",
    "It can be used for a wide range of purposes, from data mining to monitoring and automated testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALLATION\n",
    "\n",
    "you can install Scrapy and its dependencies from PyPI with:\n",
    "\n",
    "> pip install Scrapy\n",
    "\n",
    "For more information see [Installation documentation](https://docs.scrapy.org/en/latest/intro/install.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLE SPIDER CODE\n",
    "\n",
    "\n",
    "```\n",
    "# file_name = quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com/tag/humor/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'author': quote.xpath('span/small/text()').get(),\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(\"href\")').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to run your scrapy spider:\n",
    "> scrapy runspider quotes_spider.py -o quotes.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What just happened?\n",
    "\n",
    "When you ran the command `scrapy runspider quotes_spider.py`, Scrapy looked for a Spider definition inside it and ran it through its crawler engine.\n",
    "\n",
    "The crawl started by making requests to the URLs defined in the start_urls attribute (in this case, only the URL for quotes in humor category) and called the default callback method parse, passing the response object as an argument. In the parse callback, we loop through the quote elements using a CSS Selector, yield a Python dict with the extracted quote text and author, look for a link to the next page and schedule another request using the same parse method as callback.\n",
    "\n",
    "Here you notice one of the main advantages about Scrapy: requests are scheduled and processed asynchronously. This means that Scrapy doesn’t need to wait for a request to be finished and processed, it can send another request or do other things in the meantime. This also means that other requests can keep going even if some request fails or an error happens while handling it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplest way to dump all my scraped items into a JSON/CSV/XML file?\n",
    "\n",
    "To dump into a JSON file:\n",
    "\n",
    "> scrapy crawl myspider -O items.json\n",
    "\n",
    "To dump into a CSV file:\n",
    "\n",
    "> scrapy crawl myspider -O items.csv\n",
    "\n",
    "To dump into a XML file:\n",
    "\n",
    "> scrapy crawl myspider -O items.xml\n",
    "\n",
    "For more information see [Feed exports](https://docs.scrapy.org/en/latest/topics/feed-exports.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrapy project example : [quotesbot](https://github.com/scrapy/quotesbot)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn to Extract data\n",
    "\n",
    "The best way to learn how to extract data with Scrapy is trying selectors using the Scrapy shell. \n",
    "\n",
    "Run:\n",
    "\n",
    "> scrapy shell 'https://quotes.toscrape.com/page/1/'\n",
    "\n",
    "Using the shell, you can try selecting elements using CSS with the response object:\n",
    "\n",
    "> ->>> response.css('title')\n",
    "\n",
    "> [< Selector xpath='descendant-or-self::title' data='< title >Quotes to Scrape</ title>'>]\n",
    "\n",
    "The result of running response.css('title') is a list-like object called SelectorList, which represents a list of Selector objects that wrap around XML/HTML elements and allow you to run further queries to fine-grain the selection or extract the data.\n",
    "\n",
    "To extract the text from the title above, you can do:\n",
    "\n",
    "> ->>>response.css('title::text').getall()\n",
    "\n",
    "> ['Quotes to Scrape']\n",
    "\n",
    "There are two things to note here: one is that we’ve added ::text to the CSS query, to mean we want to select only the text elements directly inside < title> element. \n",
    "\n",
    "The other thing is that the result of calling .getall() is a list: it is possible that a selector returns more than one result, so we extract them all. When you know you just want the first result, as in this case, you can do:\n",
    "\n",
    "> ->>>response.css('title::text').get()\n",
    "\n",
    "> 'Quotes to Scrape'\n",
    "\n",
    "As an alternative, you could’ve written:\n",
    "\n",
    "> ->>>response.css('title::text')[0].get()\n",
    "\n",
    "> 'Quotes to Scrape'\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Scrapy from a script\n",
    "\n",
    "You can use the API to run Scrapy from a script, instead of the typical way of running Scrapy via `scrapy crawl`.\n",
    "\n",
    "Remember that Scrapy is built on top of the Twisted asynchronous networking library, so you need to run it inside the Twisted reactor.\n",
    "\n",
    "The first utility you can use to run your spiders is `scrapy.crawler.CrawlerProcess`. \n",
    "\n",
    "This class will start a Twisted reactor for you, configuring the logging and setting shutdown handlers. This class is the one used by all Scrapy commands.\n",
    "\n",
    "Note that you will also have to shutdown the Twisted reactor yourself after the spider is finished. This can be achieved by adding callbacks to the deferred returned by the `CrawlerRunner.crawl` method.\n",
    "\n",
    "Here’s an example of its usage, along with a callback to manually stop the reactor after MySpider has finished running.\n",
    "\n",
    "```\n",
    "from twisted.internet import reactor\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.log import configure_logging\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    # Your spider definition\n",
    "    ...\n",
    "\n",
    "configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})\n",
    "runner = CrawlerRunner()\n",
    "\n",
    "d = runner.crawl(MySpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run() # the script will block here until the crawling is finished\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
